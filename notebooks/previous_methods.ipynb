{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment(path):\n",
    "    df = pd.read_csv(path, index_col='Unnamed: 0')\n",
    "    columns = ['train_BA', 'train_auroc', 'val_BA', 'val_auroc', 'test_BA', 'test_auroc']\n",
    "    for column in columns:\n",
    "        df[column] = df[column].apply(lambda string: np.fromstring(string[1:-1], sep=' '))\n",
    "    return df\n",
    "\n",
    "def split_df(df, index):\n",
    "    X_train = torch.Tensor(df[df.n<=360]['n'].to_numpy())\n",
    "    y_train = torch.Tensor(np.array(df[df.n<=360]['test_auroc'].to_list())[:,index])\n",
    "    X_test = torch.Tensor(df[df.n>360]['n'].to_numpy())\n",
    "    y_test = torch.Tensor(np.array(df[df.n>360]['test_auroc'].to_list())[:,index])\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class power_law(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.epsilon = 0.0\n",
    "        self.theta1 = torch.nn.Parameter(torch.tensor([0.0]))\n",
    "        self.theta2 = torch.nn.Parameter(torch.tensor([0.0]))\n",
    "        self.softplus = torch.nn.Softplus()\n",
    "\n",
    "    def forward(self, x):\n",
    "        theta1 = self.softplus(self.theta1)\n",
    "        theta2 = torch.sigmoid(self.theta2)\n",
    "        return (1.0 - self.epsilon) - (theta1 * torch.pow(x, theta2)).ravel()\n",
    "    \n",
    "def train_power_law(X, y, training_iter=100000):\n",
    "    model = power_law()\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_func = nn.MSELoss()\n",
    "    for i in range(training_iter):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "        loss = loss_func(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A review of [Jain et al.'s (2023)](https://arxiv.org/abs/2303.01598) prediction model for our setting (not few-shot setting).\n",
    "\n",
    "They define\n",
    "\n",
    "\\begin{align}\n",
    "1-\\hat{v}(n_i) &= \\theta_1 n^{\\theta_2} \\\\\n",
    "\\hat{v}(n) &= 1 - \\theta_1 n^{\\theta_2}\n",
    "\\end{align}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y}_i &= \\log(1-\\hat{v}(n_i)) \\\\\n",
    "&= \\log(1 - (1 - \\theta_1 n^{\\theta_2})) \\\\\n",
    "&= \\log(1 - 1 + \\theta_1 n^{\\theta_2}) \\\\\n",
    "&= \\log(\\theta_1) + \\theta_2\\log(n).\n",
    "\\end{align}\n",
    "\n",
    "Then they represent their model in the form\n",
    "\n",
    "\\begin{align}\n",
    "\\alpha(x) &= [1, \\log(n)] \\\\\n",
    "\\theta &= [\\log(\\theta_1), \\theta_2]^T.\n",
    "\\end{align}\n",
    "\n",
    "Note: both $\\alpha$ and $\\theta$ should be bold.\n",
    "\n",
    "They define\n",
    "\n",
    "\\begin{align}\n",
    "\\Sigma_\\theta = (J^TJ)^{-1}, J = \\left[ \\frac{\\partial \\mathbf{\\hat{y}}}{\\partial \\theta} \\right]\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "where $J \\in \\mathcal{R}^{m\\times2}$, $m$ is the number of training samples, and\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathbf{\\hat{y}}}{\\partial \\theta_1} &= \\frac{1}{\\theta_1} \\\\\n",
    "\\frac{\\partial \\mathbf{\\hat{y}}}{\\partial \\theta_2} &= \\log(n).\n",
    "\\end{align}\n",
    "\n",
    "They define the mean and variance for the log domain as\n",
    "\n",
    "\\begin{align}\n",
    "\\mu_{\\hat{y}}(n) &= \\hat{y} \\\\\n",
    "\\sigma^2_{\\hat{y}}(n) &= \\alpha^T(n)\\Sigma_\\theta\\alpha(n).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>random_state</th>\n",
       "      <th>train_BA</th>\n",
       "      <th>train_auroc</th>\n",
       "      <th>val_BA</th>\n",
       "      <th>val_auroc</th>\n",
       "      <th>test_BA</th>\n",
       "      <th>test_auroc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>mean</td>\n",
       "      <td>[[0.79979571, 0.73333333, 0.73013219], [0.8212...</td>\n",
       "      <td>[[0.81154239, 0.79803922, 0.787068], [0.872838...</td>\n",
       "      <td>[[0.63507077, 0.67985708, 0.54648319], [0.6245...</td>\n",
       "      <td>[[0.66493573, 0.73191571, 0.55944552], [0.6650...</td>\n",
       "      <td>[[0.65107024, 0.69175081, 0.55387147], [0.6484...</td>\n",
       "      <td>[0.6858934266666666, 0.73942905, 0.59523719]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>240</td>\n",
       "      <td>mean</td>\n",
       "      <td>[[0.76419842, 0.72886922, 0.703125], [0.780733...</td>\n",
       "      <td>[[0.79834651, 0.77641348, 0.76475694], [0.8513...</td>\n",
       "      <td>[[0.63221494, 0.67773304, 0.56368396], [0.6325...</td>\n",
       "      <td>[[0.66143413, 0.72811977, 0.5764621], [0.66903...</td>\n",
       "      <td>[[0.64416552, 0.68318055, 0.56370069], [0.6441...</td>\n",
       "      <td>[0.6810822633333333, 0.7379454033333334, 0.600...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>280</td>\n",
       "      <td>mean</td>\n",
       "      <td>[[0.76189431, 0.73673674, 0.68069986], [0.7611...</td>\n",
       "      <td>[[0.78875714, 0.78656434, 0.75529881], [0.8432...</td>\n",
       "      <td>[[0.63441532, 0.68058398, 0.5519029], [0.63858...</td>\n",
       "      <td>[[0.66500938, 0.73349715, 0.56759572], [0.6778...</td>\n",
       "      <td>[[0.64824932, 0.68792408, 0.56802236], [0.6514...</td>\n",
       "      <td>[0.6881680066666668, 0.7443241866666667, 0.600...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>320</td>\n",
       "      <td>mean</td>\n",
       "      <td>[[0.73260479, 0.75357143, 0.67634027], [0.7671...</td>\n",
       "      <td>[[0.78587266, 0.79214286, 0.73871764], [0.8453...</td>\n",
       "      <td>[[0.63399048, 0.67859005, 0.57828413], [0.6385...</td>\n",
       "      <td>[[0.66374208, 0.73379837, 0.60318726], [0.6781...</td>\n",
       "      <td>[[0.64981552, 0.68217625, 0.58400504], [0.6465...</td>\n",
       "      <td>[0.6842838533333334, 0.7463586633333333, 0.609...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>360</td>\n",
       "      <td>mean</td>\n",
       "      <td>[[0.74019287, 0.73516129, 0.66513963], [0.7559...</td>\n",
       "      <td>[[0.78228179, 0.77954839, 0.71704274], [0.8318...</td>\n",
       "      <td>[[0.6297232, 0.67233627, 0.58974225], [0.64571...</td>\n",
       "      <td>[[0.6610841, 0.72788237, 0.6147395], [0.685054...</td>\n",
       "      <td>[[0.6389557, 0.67894272, 0.59206388], [0.65201...</td>\n",
       "      <td>[0.6880869333333334, 0.7448506900000001, 0.616...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     n random_state                                           train_BA  \\\n",
       "0  200         mean  [[0.79979571, 0.73333333, 0.73013219], [0.8212...   \n",
       "1  240         mean  [[0.76419842, 0.72886922, 0.703125], [0.780733...   \n",
       "2  280         mean  [[0.76189431, 0.73673674, 0.68069986], [0.7611...   \n",
       "3  320         mean  [[0.73260479, 0.75357143, 0.67634027], [0.7671...   \n",
       "4  360         mean  [[0.74019287, 0.73516129, 0.66513963], [0.7559...   \n",
       "\n",
       "                                         train_auroc  \\\n",
       "0  [[0.81154239, 0.79803922, 0.787068], [0.872838...   \n",
       "1  [[0.79834651, 0.77641348, 0.76475694], [0.8513...   \n",
       "2  [[0.78875714, 0.78656434, 0.75529881], [0.8432...   \n",
       "3  [[0.78587266, 0.79214286, 0.73871764], [0.8453...   \n",
       "4  [[0.78228179, 0.77954839, 0.71704274], [0.8318...   \n",
       "\n",
       "                                              val_BA  \\\n",
       "0  [[0.63507077, 0.67985708, 0.54648319], [0.6245...   \n",
       "1  [[0.63221494, 0.67773304, 0.56368396], [0.6325...   \n",
       "2  [[0.63441532, 0.68058398, 0.5519029], [0.63858...   \n",
       "3  [[0.63399048, 0.67859005, 0.57828413], [0.6385...   \n",
       "4  [[0.6297232, 0.67233627, 0.58974225], [0.64571...   \n",
       "\n",
       "                                           val_auroc  \\\n",
       "0  [[0.66493573, 0.73191571, 0.55944552], [0.6650...   \n",
       "1  [[0.66143413, 0.72811977, 0.5764621], [0.66903...   \n",
       "2  [[0.66500938, 0.73349715, 0.56759572], [0.6778...   \n",
       "3  [[0.66374208, 0.73379837, 0.60318726], [0.6781...   \n",
       "4  [[0.6610841, 0.72788237, 0.6147395], [0.685054...   \n",
       "\n",
       "                                             test_BA  \\\n",
       "0  [[0.65107024, 0.69175081, 0.55387147], [0.6484...   \n",
       "1  [[0.64416552, 0.68318055, 0.56370069], [0.6441...   \n",
       "2  [[0.64824932, 0.68792408, 0.56802236], [0.6514...   \n",
       "3  [[0.64981552, 0.68217625, 0.58400504], [0.6465...   \n",
       "4  [[0.6389557, 0.67894272, 0.59206388], [0.65201...   \n",
       "\n",
       "                                          test_auroc  \n",
       "0       [0.6858934266666666, 0.73942905, 0.59523719]  \n",
       "1  [0.6810822633333333, 0.7379454033333334, 0.600...  \n",
       "2  [0.6881680066666668, 0.7443241866666667, 0.600...  \n",
       "3  [0.6842838533333334, 0.7463586633333333, 0.609...  \n",
       "4  [0.6880869333333334, 0.7448506900000001, 0.616...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ChestX-ray14 (long range)\n",
    "df = load_experiment('/cluster/home/eharve06/extrapolating-classifier-accuracy-to-bigger-datasets/experiments/ChestX-ray14_long_range.csv')\n",
    "df = df.groupby('n').agg(lambda x: list(x))\n",
    "df.test_auroc = df.test_auroc.apply(lambda x: np.mean(x, axis=0))\n",
    "df.random_state = df.random_state.apply(lambda x: 'mean')\n",
    "df = df.reset_index()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "power_law(\n",
       "  (softplus): Softplus(beta=1, threshold=20)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 0\n",
    "X_train, y_train, X_test, y_test = split_df(df, index=index)\n",
    "model = train_power_law(X_train, y_train)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thetas:\n",
      "[[-1.15679336e+00]\n",
      " [ 1.62080607e-06]]\n",
      "\n",
      "J:\n",
      "[[3.1797207  5.29831743]\n",
      " [3.1797207  5.48063898]\n",
      " [3.1797207  5.63478947]\n",
      " [3.1797207  5.76832104]\n",
      " [3.1797207  5.88610411]]\n",
      "\n",
      "Sigma:\n",
      "[[14.4676618  -8.18368698]\n",
      " [-8.18368698  4.63547107]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "theta1, theta2 = [p for p in model.parameters()]\n",
    "theta1, theta2 = model.softplus(theta1).item(), torch.sigmoid(theta2).item()\n",
    "thetas = np.array([[np.log(theta1)], [theta2]])\n",
    "\n",
    "J = np.array([[1/theta1, np.log(x)] for x in X_train.detach().numpy()])\n",
    "Sigma = np.linalg.inv(J.T@J)\n",
    "\n",
    "print('Thetas:\\n{}\\n'.format(thetas))\n",
    "print('J:\\n{}\\n'.format(J))\n",
    "print('Sigma:\\n{}\\n'.format(Sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.stats import norm, lognorm\n",
    "\n",
    "alphas = np.array([[[1], [np.log(x)]] for x in np.linspace(200, 30000, 1000)])\n",
    "y_means = np.array([alpha.T@thetas for alpha in alphas]).ravel()\n",
    "y_sigmas = np.array([alpha.T@Sigma@alpha for alpha in alphas]).ravel()\n",
    "\n",
    "v_means = 1-np.exp(y_means+y_sigmas/2)\n",
    "v_sigmas = np.exp(y_means+y_sigmas/2)*np.sqrt(np.exp(y_sigmas-1))\n",
    "\n",
    "sigmas = [lognorm(s=np.sqrt(v_sigma), loc=v_mean).std() for v_mean, v_sigma in zip(v_means, v_sigmas)]\n",
    "means = [lognorm(s=np.sqrt(v_sigma), loc=v_mean).mean() for v_mean, v_sigma in zip(v_means, v_sigmas)]\n",
    "#lowers = [lognorm(s=np.sqrt(v_sigma), loc=v_mean).ppf(0.025) for v_mean, v_sigma in zip(v_means, v_sigmas)]\n",
    "#uppers = [lognorm(s=np.sqrt(v_sigma), loc=v_mean).ppf(0.975) for v_mean, v_sigma in zip(v_means, v_sigmas)]\n",
    "\n",
    "#uppers = 1-np.exp([scipy.stats.norm.ppf(0.05, loc=y_mean, scale=np.sqrt(y_sigma)) for y_mean, y_sigma in zip(y_means, y_sigmas)])\n",
    "#means = 1-np.exp([scipy.stats.norm(loc=y_mean, scale=np.sqrt(y_sigma)).mean() for y_mean, y_sigma in zip(y_means, y_sigmas)])\n",
    "#lowers = 1-np.exp([scipy.stats.norm.ppf(0.6, loc=y_mean, scale=np.sqrt(y_sigma)) for y_mean, y_sigma in zip(y_means, y_sigmas)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68550427, 0.6855042 , 0.68550414, 0.68550408, 0.68550403,\n",
       "       0.68550398, 0.68550394, 0.6855039 , 0.68550387, 0.68550384,\n",
       "       0.6855038 , 0.68550377, 0.68550375, 0.68550372, 0.68550369,\n",
       "       0.68550367, 0.68550365, 0.68550363, 0.6855036 , 0.68550358,\n",
       "       0.68550356, 0.68550355, 0.68550353, 0.68550351, 0.68550349,\n",
       "       0.68550348, 0.68550346, 0.68550345, 0.68550343, 0.68550342,\n",
       "       0.6855034 , 0.68550339, 0.68550338, 0.68550336, 0.68550335,\n",
       "       0.68550334, 0.68550333, 0.68550331, 0.6855033 , 0.68550329,\n",
       "       0.68550328, 0.68550327, 0.68550326, 0.68550325, 0.68550324,\n",
       "       0.68550323, 0.68550322, 0.68550321, 0.6855032 , 0.68550319,\n",
       "       0.68550318, 0.68550317, 0.68550316, 0.68550315, 0.68550315,\n",
       "       0.68550314, 0.68550313, 0.68550312, 0.68550311, 0.68550311,\n",
       "       0.6855031 , 0.68550309, 0.68550308, 0.68550308, 0.68550307,\n",
       "       0.68550306, 0.68550305, 0.68550305, 0.68550304, 0.68550303,\n",
       "       0.68550303, 0.68550302, 0.68550301, 0.68550301, 0.685503  ,\n",
       "       0.68550299, 0.68550299, 0.68550298, 0.68550298, 0.68550297,\n",
       "       0.68550296, 0.68550296, 0.68550295, 0.68550295, 0.68550294,\n",
       "       0.68550294, 0.68550293, 0.68550292, 0.68550292, 0.68550291,\n",
       "       0.68550291, 0.6855029 , 0.6855029 , 0.68550289, 0.68550289,\n",
       "       0.68550288, 0.68550288, 0.68550287, 0.68550287, 0.68550286,\n",
       "       0.68550286, 0.68550285, 0.68550285, 0.68550284, 0.68550284,\n",
       "       0.68550284, 0.68550283, 0.68550283, 0.68550282, 0.68550282,\n",
       "       0.68550281, 0.68550281, 0.6855028 , 0.6855028 , 0.6855028 ,\n",
       "       0.68550279, 0.68550279, 0.68550278, 0.68550278, 0.68550277,\n",
       "       0.68550277, 0.68550277, 0.68550276, 0.68550276, 0.68550275,\n",
       "       0.68550275, 0.68550275, 0.68550274, 0.68550274, 0.68550274,\n",
       "       0.68550273, 0.68550273, 0.68550272, 0.68550272, 0.68550272,\n",
       "       0.68550271, 0.68550271, 0.68550271, 0.6855027 , 0.6855027 ,\n",
       "       0.6855027 , 0.68550269, 0.68550269, 0.68550269, 0.68550268,\n",
       "       0.68550268, 0.68550268, 0.68550267, 0.68550267, 0.68550267,\n",
       "       0.68550266, 0.68550266, 0.68550266, 0.68550265, 0.68550265,\n",
       "       0.68550265, 0.68550264, 0.68550264, 0.68550264, 0.68550263,\n",
       "       0.68550263, 0.68550263, 0.68550262, 0.68550262, 0.68550262,\n",
       "       0.68550262, 0.68550261, 0.68550261, 0.68550261, 0.6855026 ,\n",
       "       0.6855026 , 0.6855026 , 0.6855026 , 0.68550259, 0.68550259,\n",
       "       0.68550259, 0.68550258, 0.68550258, 0.68550258, 0.68550258,\n",
       "       0.68550257, 0.68550257, 0.68550257, 0.68550257, 0.68550256,\n",
       "       0.68550256, 0.68550256, 0.68550255, 0.68550255, 0.68550255,\n",
       "       0.68550255, 0.68550254, 0.68550254, 0.68550254, 0.68550254,\n",
       "       0.68550253, 0.68550253, 0.68550253, 0.68550253, 0.68550252,\n",
       "       0.68550252, 0.68550252, 0.68550252, 0.68550251, 0.68550251,\n",
       "       0.68550251, 0.68550251, 0.6855025 , 0.6855025 , 0.6855025 ,\n",
       "       0.6855025 , 0.68550249, 0.68550249, 0.68550249, 0.68550249,\n",
       "       0.68550249, 0.68550248, 0.68550248, 0.68550248, 0.68550248,\n",
       "       0.68550247, 0.68550247, 0.68550247, 0.68550247, 0.68550247,\n",
       "       0.68550246, 0.68550246, 0.68550246, 0.68550246, 0.68550245,\n",
       "       0.68550245, 0.68550245, 0.68550245, 0.68550245, 0.68550244,\n",
       "       0.68550244, 0.68550244, 0.68550244, 0.68550244, 0.68550243,\n",
       "       0.68550243, 0.68550243, 0.68550243, 0.68550242, 0.68550242,\n",
       "       0.68550242, 0.68550242, 0.68550242, 0.68550241, 0.68550241,\n",
       "       0.68550241, 0.68550241, 0.68550241, 0.6855024 , 0.6855024 ,\n",
       "       0.6855024 , 0.6855024 , 0.6855024 , 0.6855024 , 0.68550239,\n",
       "       0.68550239, 0.68550239, 0.68550239, 0.68550239, 0.68550238,\n",
       "       0.68550238, 0.68550238, 0.68550238, 0.68550238, 0.68550237,\n",
       "       0.68550237, 0.68550237, 0.68550237, 0.68550237, 0.68550237,\n",
       "       0.68550236, 0.68550236, 0.68550236, 0.68550236, 0.68550236,\n",
       "       0.68550235, 0.68550235, 0.68550235, 0.68550235, 0.68550235,\n",
       "       0.68550235, 0.68550234, 0.68550234, 0.68550234, 0.68550234,\n",
       "       0.68550234, 0.68550234, 0.68550233, 0.68550233, 0.68550233,\n",
       "       0.68550233, 0.68550233, 0.68550233, 0.68550232, 0.68550232,\n",
       "       0.68550232, 0.68550232, 0.68550232, 0.68550232, 0.68550231,\n",
       "       0.68550231, 0.68550231, 0.68550231, 0.68550231, 0.68550231,\n",
       "       0.6855023 , 0.6855023 , 0.6855023 , 0.6855023 , 0.6855023 ,\n",
       "       0.6855023 , 0.68550229, 0.68550229, 0.68550229, 0.68550229,\n",
       "       0.68550229, 0.68550229, 0.68550228, 0.68550228, 0.68550228,\n",
       "       0.68550228, 0.68550228, 0.68550228, 0.68550228, 0.68550227,\n",
       "       0.68550227, 0.68550227, 0.68550227, 0.68550227, 0.68550227,\n",
       "       0.68550227, 0.68550226, 0.68550226, 0.68550226, 0.68550226,\n",
       "       0.68550226, 0.68550226, 0.68550225, 0.68550225, 0.68550225,\n",
       "       0.68550225, 0.68550225, 0.68550225, 0.68550225, 0.68550224,\n",
       "       0.68550224, 0.68550224, 0.68550224, 0.68550224, 0.68550224,\n",
       "       0.68550224, 0.68550223, 0.68550223, 0.68550223, 0.68550223,\n",
       "       0.68550223, 0.68550223, 0.68550223, 0.68550222, 0.68550222,\n",
       "       0.68550222, 0.68550222, 0.68550222, 0.68550222, 0.68550222,\n",
       "       0.68550222, 0.68550221, 0.68550221, 0.68550221, 0.68550221,\n",
       "       0.68550221, 0.68550221, 0.68550221, 0.6855022 , 0.6855022 ,\n",
       "       0.6855022 , 0.6855022 , 0.6855022 , 0.6855022 , 0.6855022 ,\n",
       "       0.6855022 , 0.68550219, 0.68550219, 0.68550219, 0.68550219,\n",
       "       0.68550219, 0.68550219, 0.68550219, 0.68550219, 0.68550218,\n",
       "       0.68550218, 0.68550218, 0.68550218, 0.68550218, 0.68550218,\n",
       "       0.68550218, 0.68550218, 0.68550217, 0.68550217, 0.68550217,\n",
       "       0.68550217, 0.68550217, 0.68550217, 0.68550217, 0.68550217,\n",
       "       0.68550216, 0.68550216, 0.68550216, 0.68550216, 0.68550216,\n",
       "       0.68550216, 0.68550216, 0.68550216, 0.68550215, 0.68550215,\n",
       "       0.68550215, 0.68550215, 0.68550215, 0.68550215, 0.68550215,\n",
       "       0.68550215, 0.68550214, 0.68550214, 0.68550214, 0.68550214,\n",
       "       0.68550214, 0.68550214, 0.68550214, 0.68550214, 0.68550214,\n",
       "       0.68550213, 0.68550213, 0.68550213, 0.68550213, 0.68550213,\n",
       "       0.68550213, 0.68550213, 0.68550213, 0.68550213, 0.68550212,\n",
       "       0.68550212, 0.68550212, 0.68550212, 0.68550212, 0.68550212,\n",
       "       0.68550212, 0.68550212, 0.68550212, 0.68550211, 0.68550211,\n",
       "       0.68550211, 0.68550211, 0.68550211, 0.68550211, 0.68550211,\n",
       "       0.68550211, 0.68550211, 0.6855021 , 0.6855021 , 0.6855021 ,\n",
       "       0.6855021 , 0.6855021 , 0.6855021 , 0.6855021 , 0.6855021 ,\n",
       "       0.6855021 , 0.68550209, 0.68550209, 0.68550209, 0.68550209,\n",
       "       0.68550209, 0.68550209, 0.68550209, 0.68550209, 0.68550209,\n",
       "       0.68550208, 0.68550208, 0.68550208, 0.68550208, 0.68550208,\n",
       "       0.68550208, 0.68550208, 0.68550208, 0.68550208, 0.68550208,\n",
       "       0.68550207, 0.68550207, 0.68550207, 0.68550207, 0.68550207,\n",
       "       0.68550207, 0.68550207, 0.68550207, 0.68550207, 0.68550207,\n",
       "       0.68550206, 0.68550206, 0.68550206, 0.68550206, 0.68550206,\n",
       "       0.68550206, 0.68550206, 0.68550206, 0.68550206, 0.68550206,\n",
       "       0.68550205, 0.68550205, 0.68550205, 0.68550205, 0.68550205,\n",
       "       0.68550205, 0.68550205, 0.68550205, 0.68550205, 0.68550205,\n",
       "       0.68550204, 0.68550204, 0.68550204, 0.68550204, 0.68550204,\n",
       "       0.68550204, 0.68550204, 0.68550204, 0.68550204, 0.68550204,\n",
       "       0.68550203, 0.68550203, 0.68550203, 0.68550203, 0.68550203,\n",
       "       0.68550203, 0.68550203, 0.68550203, 0.68550203, 0.68550203,\n",
       "       0.68550203, 0.68550202, 0.68550202, 0.68550202, 0.68550202,\n",
       "       0.68550202, 0.68550202, 0.68550202, 0.68550202, 0.68550202,\n",
       "       0.68550202, 0.68550202, 0.68550201, 0.68550201, 0.68550201,\n",
       "       0.68550201, 0.68550201, 0.68550201, 0.68550201, 0.68550201,\n",
       "       0.68550201, 0.68550201, 0.68550201, 0.685502  , 0.685502  ,\n",
       "       0.685502  , 0.685502  , 0.685502  , 0.685502  , 0.685502  ,\n",
       "       0.685502  , 0.685502  , 0.685502  , 0.685502  , 0.68550199,\n",
       "       0.68550199, 0.68550199, 0.68550199, 0.68550199, 0.68550199,\n",
       "       0.68550199, 0.68550199, 0.68550199, 0.68550199, 0.68550199,\n",
       "       0.68550199, 0.68550198, 0.68550198, 0.68550198, 0.68550198,\n",
       "       0.68550198, 0.68550198, 0.68550198, 0.68550198, 0.68550198,\n",
       "       0.68550198, 0.68550198, 0.68550197, 0.68550197, 0.68550197,\n",
       "       0.68550197, 0.68550197, 0.68550197, 0.68550197, 0.68550197,\n",
       "       0.68550197, 0.68550197, 0.68550197, 0.68550197, 0.68550196,\n",
       "       0.68550196, 0.68550196, 0.68550196, 0.68550196, 0.68550196,\n",
       "       0.68550196, 0.68550196, 0.68550196, 0.68550196, 0.68550196,\n",
       "       0.68550196, 0.68550196, 0.68550195, 0.68550195, 0.68550195,\n",
       "       0.68550195, 0.68550195, 0.68550195, 0.68550195, 0.68550195,\n",
       "       0.68550195, 0.68550195, 0.68550195, 0.68550195, 0.68550194,\n",
       "       0.68550194, 0.68550194, 0.68550194, 0.68550194, 0.68550194,\n",
       "       0.68550194, 0.68550194, 0.68550194, 0.68550194, 0.68550194,\n",
       "       0.68550194, 0.68550194, 0.68550193, 0.68550193, 0.68550193,\n",
       "       0.68550193, 0.68550193, 0.68550193, 0.68550193, 0.68550193,\n",
       "       0.68550193, 0.68550193, 0.68550193, 0.68550193, 0.68550193,\n",
       "       0.68550192, 0.68550192, 0.68550192, 0.68550192, 0.68550192,\n",
       "       0.68550192, 0.68550192, 0.68550192, 0.68550192, 0.68550192,\n",
       "       0.68550192, 0.68550192, 0.68550192, 0.68550191, 0.68550191,\n",
       "       0.68550191, 0.68550191, 0.68550191, 0.68550191, 0.68550191,\n",
       "       0.68550191, 0.68550191, 0.68550191, 0.68550191, 0.68550191,\n",
       "       0.68550191, 0.6855019 , 0.6855019 , 0.6855019 , 0.6855019 ,\n",
       "       0.6855019 , 0.6855019 , 0.6855019 , 0.6855019 , 0.6855019 ,\n",
       "       0.6855019 , 0.6855019 , 0.6855019 , 0.6855019 , 0.6855019 ,\n",
       "       0.68550189, 0.68550189, 0.68550189, 0.68550189, 0.68550189,\n",
       "       0.68550189, 0.68550189, 0.68550189, 0.68550189, 0.68550189,\n",
       "       0.68550189, 0.68550189, 0.68550189, 0.68550189, 0.68550188,\n",
       "       0.68550188, 0.68550188, 0.68550188, 0.68550188, 0.68550188,\n",
       "       0.68550188, 0.68550188, 0.68550188, 0.68550188, 0.68550188,\n",
       "       0.68550188, 0.68550188, 0.68550188, 0.68550187, 0.68550187,\n",
       "       0.68550187, 0.68550187, 0.68550187, 0.68550187, 0.68550187,\n",
       "       0.68550187, 0.68550187, 0.68550187, 0.68550187, 0.68550187,\n",
       "       0.68550187, 0.68550187, 0.68550187, 0.68550186, 0.68550186,\n",
       "       0.68550186, 0.68550186, 0.68550186, 0.68550186, 0.68550186,\n",
       "       0.68550186, 0.68550186, 0.68550186, 0.68550186, 0.68550186,\n",
       "       0.68550186, 0.68550186, 0.68550186, 0.68550185, 0.68550185,\n",
       "       0.68550185, 0.68550185, 0.68550185, 0.68550185, 0.68550185,\n",
       "       0.68550185, 0.68550185, 0.68550185, 0.68550185, 0.68550185,\n",
       "       0.68550185, 0.68550185, 0.68550185, 0.68550184, 0.68550184,\n",
       "       0.68550184, 0.68550184, 0.68550184, 0.68550184, 0.68550184,\n",
       "       0.68550184, 0.68550184, 0.68550184, 0.68550184, 0.68550184,\n",
       "       0.68550184, 0.68550184, 0.68550184, 0.68550183, 0.68550183,\n",
       "       0.68550183, 0.68550183, 0.68550183, 0.68550183, 0.68550183,\n",
       "       0.68550183, 0.68550183, 0.68550183, 0.68550183, 0.68550183,\n",
       "       0.68550183, 0.68550183, 0.68550183, 0.68550183, 0.68550182,\n",
       "       0.68550182, 0.68550182, 0.68550182, 0.68550182, 0.68550182,\n",
       "       0.68550182, 0.68550182, 0.68550182, 0.68550182, 0.68550182,\n",
       "       0.68550182, 0.68550182, 0.68550182, 0.68550182, 0.68550182,\n",
       "       0.68550181, 0.68550181, 0.68550181, 0.68550181, 0.68550181,\n",
       "       0.68550181, 0.68550181, 0.68550181, 0.68550181, 0.68550181,\n",
       "       0.68550181, 0.68550181, 0.68550181, 0.68550181, 0.68550181,\n",
       "       0.68550181, 0.6855018 , 0.6855018 , 0.6855018 , 0.6855018 ,\n",
       "       0.6855018 , 0.6855018 , 0.6855018 , 0.6855018 , 0.6855018 ,\n",
       "       0.6855018 , 0.6855018 , 0.6855018 , 0.6855018 , 0.6855018 ,\n",
       "       0.6855018 , 0.6855018 , 0.6855018 , 0.68550179, 0.68550179,\n",
       "       0.68550179, 0.68550179, 0.68550179, 0.68550179, 0.68550179,\n",
       "       0.68550179, 0.68550179, 0.68550179, 0.68550179, 0.68550179,\n",
       "       0.68550179, 0.68550179, 0.68550179, 0.68550179, 0.68550179,\n",
       "       0.68550178, 0.68550178, 0.68550178, 0.68550178, 0.68550178,\n",
       "       0.68550178, 0.68550178, 0.68550178, 0.68550178, 0.68550178,\n",
       "       0.68550178, 0.68550178, 0.68550178, 0.68550178, 0.68550178,\n",
       "       0.68550178, 0.68550178, 0.68550178, 0.68550177, 0.68550177,\n",
       "       0.68550177, 0.68550177, 0.68550177, 0.68550177, 0.68550177,\n",
       "       0.68550177, 0.68550177, 0.68550177, 0.68550177, 0.68550177,\n",
       "       0.68550177, 0.68550177, 0.68550177, 0.68550177, 0.68550177,\n",
       "       0.68550176, 0.68550176, 0.68550176, 0.68550176, 0.68550176,\n",
       "       0.68550176, 0.68550176, 0.68550176, 0.68550176, 0.68550176,\n",
       "       0.68550176, 0.68550176, 0.68550176, 0.68550176, 0.68550176,\n",
       "       0.68550176, 0.68550176, 0.68550176, 0.68550175, 0.68550175,\n",
       "       0.68550175, 0.68550175, 0.68550175, 0.68550175, 0.68550175,\n",
       "       0.68550175, 0.68550175, 0.68550175, 0.68550175, 0.68550175,\n",
       "       0.68550175, 0.68550175, 0.68550175, 0.68550175, 0.68550175,\n",
       "       0.68550175, 0.68550175, 0.68550174, 0.68550174, 0.68550174,\n",
       "       0.68550174, 0.68550174, 0.68550174, 0.68550174, 0.68550174,\n",
       "       0.68550174, 0.68550174, 0.68550174, 0.68550174, 0.68550174,\n",
       "       0.68550174, 0.68550174, 0.68550174, 0.68550174, 0.68550174,\n",
       "       0.68550174, 0.68550173, 0.68550173, 0.68550173, 0.68550173,\n",
       "       0.68550173, 0.68550173, 0.68550173, 0.68550173, 0.68550173,\n",
       "       0.68550173, 0.68550173, 0.68550173, 0.68550173, 0.68550173,\n",
       "       0.68550173, 0.68550173, 0.68550173, 0.68550173, 0.68550173,\n",
       "       0.68550172, 0.68550172, 0.68550172, 0.68550172, 0.68550172,\n",
       "       0.68550172, 0.68550172, 0.68550172, 0.68550172, 0.68550172,\n",
       "       0.68550172, 0.68550172, 0.68550172, 0.68550172, 0.68550172,\n",
       "       0.68550172, 0.68550172, 0.68550172, 0.68550172, 0.68550171])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-np.exp(y_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ncols = 1\n",
    "nrows = 1\n",
    "fig, axs = plt.subplots(ncols=ncols, nrows=nrows, figsize=(ncols*4, nrows*3))\n",
    "# Plot data\n",
    "temp_df = df[df.random_state=='mean']\n",
    "np.array(axs).flatten()[0]\\\n",
    ".scatter(temp_df[temp_df.n<=360].n.to_numpy(),\n",
    "         np.array(temp_df[temp_df.n<=360].test_auroc.to_list())[:,index],\n",
    "         color='black',\n",
    "         alpha=1.0,\n",
    "         label='Initial subsets')\n",
    "np.array(axs).flatten()[0]\\\n",
    ".scatter(temp_df[temp_df.n>360].n.to_numpy(),\n",
    "         np.array(temp_df[temp_df.n>360].test_auroc.to_list())[:,index],\n",
    "         color='black',\n",
    "         alpha=0.3,\n",
    "         label='Ground truth')\n",
    "np.array(axs).flatten()[0]\\\n",
    "        .plot(np.linspace(200, 30000, 1000), model(torch.linspace(200, 30000, 1000)).detach().numpy(), color='#1f77b4', label='Our model')\n",
    "np.array(axs).flatten()[0]\\\n",
    "        .plot(np.linspace(200, 30000, 1000), 1-np.exp(y_means), color='#d62728', label='Power law')\n",
    "#np.array(axs).flatten()[0]\\\n",
    "#        .fill_between(np.linspace(200, 30000, 1000), lowers, uppers, color='#d62728', alpha=0.3, label='Power law')\n",
    "np.array(axs).flatten()[0]\\\n",
    ".set_ylim([0,1])\n",
    "np.array(axs).flatten()[0]\\\n",
    ".set_xscale('log')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdl_2022f_env",
   "language": "python",
   "name": "bdl_2022f_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
